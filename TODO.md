- [ ] Mini-batch SGD interface for Trainer (this is really hard for some reason)
- [ ] Improve MNIST performance by A LOT (current: 8528/10000) (batch normalization? dropout? ADAM? increasing network complexity? hyperparameter tuning)
- [ ] Add proper parallelization
- [ ] Proper evaluation framework for the network
- [ ] Optimizers, gradient boosting, momentum, etc.?
- [ ] Fully learn all the math and how to derive the backprop formulas
- [ ] Deploy and write frontend
- [ ] Write blog post

- [x] Fix issues, lr must be set to 10 in order for it to predict XOR correctly
- [x] Create a proper trainer framework for the neural net, also do some type foolery probably | type (`Trainer`?) that contains all hyperparameters (lr, epochs, etc.)
- [x] Add better documentation in the form of comments
- [x] Framework for training on MNIST
- [x] Activation function type
- [x] Implement feature to be able to save parameters to disk so there's no need to re-train the network each time
- [x] Parallelization
- [x] Train on MNIST 
